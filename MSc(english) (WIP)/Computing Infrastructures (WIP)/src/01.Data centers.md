
![](ddc10bf17a4912a39b0d63795d575414.png)


## Data centers 

Web application with thin client application configuration: 

| ADVANTAGES                                        |     DISADVANTAGES         |
| :---: | :--:|        
| Lower IT costs                                    | Require a constant Internet connection (so not work well with low-speed connections and latency problem) |
| High performance, "unlimited" storage capacity and backup | Privacy and security issues                                                          |
|       Device independence (no need to manually configure ...) |                    High Power Consumption                                                                   |


The key has been virtualization. Warehouse-scale Computer Infrastructure which powers VMs. 

Example of Warehouse-scale Computer Infrastructure: 

![](d320ff0ee0df27ca0eb0a5e55f5cce1e.png)



![](bce7fdba24a18c9e92db4f5b0d6447e5.png)



![](bd5334220f3bfdcbfdab8ee6d16d4a40.png)


---

Virtualization has been crucial to make datacenters reals. 

WSCs use a relatively homogeneous hardware and
system software platform to simplify management and reduce cost. 

Warehouse-scale Computer

![](2d7790e4bd72220a96cc3b2b5245e508.png){width=50%}


TOWER 
| PROs  | CONs| 
|---|----|
| Scalability and ease of upgrade | Consumes a lot of space |
| Cost-effective (cheapest servers) | Provides a basic level of performance |
| Cools easily (low component density) | Complicated cable management|


RACK SERVER
| PROs                        | CONs        |
| --------------------------- | ----------- |
| Failure containment         | Power usage |
| Simplified cable management |         Maintenance    |
| Cost-effective                            |             |




BLADE SERVER 

| PROs                                                               | CONs                                                 |
| ------------------------------------------------------------------ | ---------------------------------------------------- |
| Load balancing and failover                                        | Expensive configuration                              |
| Centralized management, all blades connected to a single interface | high component density -> effort to avoid overheated |
| Cabling                                                            |                                                      |
|                              Size and form-factor                                      |                                                      |

Blade servers are a new and advanced type of server that can be described as a hybrid rack server. These servers are housed in blade enclosures, creating a blade system. Blade servers are the smallest type of server available, making them ideal for conserving space.



--- 

28 feb 

In recent years, deep learning models have become increasingly popular and have driven the development of specialized hardware for machine learning applications. The compute requirements for AI training have been doubling every 3.5 months since 2013, which is faster than the expected rate of Moore's Law. To meet these growing compute demands, data centers use specialized hardware accelerators such as GPUs, TPUs, and FPGAs. These accelerators provide high performance and efficient processing for deep learning workloads. GPUs have been widely used for deep learning since they provide high processing power and are well-suited for parallel computing tasks. TPUs were specifically designed by Google to accelerate deep learning tasks and perform matrix operations more efficiently than GPUs or CPUs. FPGAs, on the other hand, offer flexibility and can be programmed for specific tasks, making them a good choice for custom hardware design.


### CPU and GPU 



![](52be8dcf266a899924c422348aec179e.png){width=65%}


The generic workflow of training a DNN on multiple GPUs consists:

![](96fe52c0c403d6eb2eeef7e7f8a4fbe4.png){width=50%}


The performance of synchronous systems is constrained by the slowest learner and the slowest messages in the network. Since the communication phase is a crucial aspect, a high-performance network can facilitate swift reconciliation of parameters across learners. In DNN training, a possible solution to the bottleneck in CPU-GPU workflow is to use NVlink, which allows GPUs to communicate without passing data to the host CPU. In a CPU-GPU configuration, a CPU host is connected to an accelerator tray with several GPUs attached via PCIe. The GPUs within the tray are linked using high-bandwidth interconnects such as NVlink.


![](391b951e059a8ef7d89b98587f051156.png){width=50%}




### TPU 

-   TPUs are specialized hardware designed for machine learning (ML) workloads that perform integer operations with reduced precision, allowing them to process large amounts of data more efficiently than GPUs or CPUs.
-   TPUs excel at performing the types of matrix operations commonly used in deep learning models.
-   By using TPUs, Google is able to train and run ML models faster and at a lower cost than using general-purpose hardware.
-   TensorFlow is a custom-built integrated circuit tailored specifically for ML and has been powering Google data centers since 2015, along with CPUs and GPUs.
-   Tensors are the basic unit of operation in TensorFlow and represent n-dimensional matrices.
-   Each Tensor core in TensorFlow has an array for matrix computations (MXU) and a high bandwidth memory (HBM) connection to store parameters and intermediate values during computation.
-   TPUv3 is the first liquid-cooled accelerator in Google’s data center.




### Field-Programmable Gate Array (FPGA)


![](208ecccf9462730f0ba1c7585b9914bb.png){width=50%}

-   Field-Programmable Gate Arrays (FPGAs) are arrays of logic gates that can be programmed by the user
-   FPGAs consist of configurable logic blocks (CLBs) that are interconnected to implement common functions with high flexibility
-   VHDL and Verilog are hardware description languages used to describe hardware using text-based schematics with components and interconnections.



![](df3f8e212c9d10cdb095e55bd90534c5.png){width=50%}


|     | Advantages | Disadvantages |
| --- | ---------- | ------------- |
| CPU | Easy to be programmed and * support any programming framework. | Most suited for simple models that do not take long to train and for small models with small training set.               |
| GPU |   Ideal for applications in which data need to be processed in parallel like the pixels of images or videos. | Programmed in languages like CUDA and OpenCL and therefore provide limited flexibility compared to CPUs. |
| TPU | Very fast at performing dense . vector and matrix computations  and are specialized on running very « fast ML workloads | For applications and models based on TensorFlow/PyTorch/ JAX Lower flexibility compared to CPUs and GPUs |
| FPGA | Higher performance, lower cost . and lower power consumption compared to other options like . CPUs and GPU | Programmed using OpenCL and High-level Synthesis (HLS). Limited flexibility compared to other platforms. |



Hardest part of AI isn’t AI

![](7ee45b4851e58caa0cd4ec8df9b4d2b8.png){width=50%}





--- 

# storage


SSD good for random reads but issues with random writes. 

HDD always more used in datacenters. 

often storage is a bottleneck


Disks can be seen by an OS as a collection of  
data blocks that can be read or written independently. 

- To allow the ordering/management among them, each block is characterized by a unique numerical address called LBA (Logical Block Address). 
- Typically, the OS groups blocks into clusters to simplify the access to the disk. Clusters, which can range from 1 disk sector (512 B) to 128 sectors (64 KB), are the minimal unit that an OS can read from or write to a disk. 

Clusters contains: 

- File data
- Meta data:
	- File names  
	- Directory structures and symbolic links  
	- File size and file type  
	- Creation, modification, last access dates  
	- Security information (owners, access list, encryption)  
	- Links to the LBA where the file content can be located on the disk

Since the file system can only access clusters, the real occupation of space on a disk for a file is always a multiple of the cluster size.

$$\text{actual size on disk}= ceil (\frac{\text{the file size}}{\text{the cluster size}})*\text{the cluster size}$$

The waste of space is called internal fragmentation of files.
$$\text{waste disk space} = \text{actual size on disk} - \text{file size}$$

Deleting a file never actually deletes the data on the disk: when a new file will be written on the same clusters, the old data will be replaced by the new one.

As the life of the disk evolves, there might not be enough space to store a file contiguously. In this case, the file is split into smaller chunks that are inserted into the free clusters spread over the disk. The effect of splitting a file into non-contiguous clusters is called external fragmentation. As we will see, this can reduce a lot the performance of an HDD.

HDD 

An HDD consists of one or more rigid ("hard") rotating disks (platters) with magnetic heads arranged on a moving actuator arm to read and write data to the surfaces.

![](1e50e0915030e7f00687f44530fa2b4b.png)

Drive geometry  
§ Sectors arranged into tracks  
§ A cylinder is a particular track on multiple platters  
§ Tracks arranged in concentric circles on platters  
§ A disk may have multiple, double-sided platters

To understand well this: 

Externally, hard drives expose a large number of sectors (blocks)  
§ Typically 512 or 4096 bytes  
§ Individual sector writes are atomic  
§ Have a header and an error correction code  
§ Multiple sectors writes may be interrupted (torn write)


![](cbad625a261fa6267408a654f3b3c385.png){width = 50%}

The external track are "faster" to read since angular velocity considerations and are more dense. 

Four types of delay  
1. Rotational Delay :Time to rotate the desired sector to the read head related to RPM 2 $T_{rotation}=\frac{R_{period}}{2}$ with $R_{period}=\frac{1}{RPM}$
2. Seek delay: Time to move the read head to a different track $T_{\text{seek AVG}}=\frac{T_{\text{seek MAX}}}{3}$ 
3. Transfer time: average time for a request (write or read) $T_{I/O}=T_{seek}+T_{rotation}+T_{transfer}+T_{overhead}$. Or, considering data locality: $T_{I/O}=(1-DL)(T_{seek}+T_{rotation})+T_{transfer}+T_{overhead}$.
4. Controller Overhead: Overhead for the request management


Data locality 








