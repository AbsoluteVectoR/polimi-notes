
# VLIW

## Static Scheduling intro

Two strategies to support ILP: Static and Dynamic scheduling. VLIW is an example of **Static Scheduling**: it relies on software for identifying potential parallelism.
There are **limits of static scheduling**:

* **Unpredictable Branches**: the compiler can't know ahead of time which branch will be taken.
* **Variable Memory Latency**: compiler can't predict which memory blocks will be in cache at any given time. So it cannot optimize the instructions that rely on the data in memory.
* **Code Size Explosion**: Static scheduling can also result in a code size explosion. As we schedule instructions, we may need to insert more instructions to handle dependencies and ensure proper execution order.
* **Compiler Complexity**: static scheduling can also increase compiler complexity. The compiler must analyze the code for dependencies and optimize the instruction scheduling accordingly.


## VLIW

In VLIW, there is no issue stage; instead, there is a fetch stage where the bundle is fetched from memory, and a decode stage where the operations within the instruction are separated and sent to functional units (FUs). As conflicts are resolved by the compiler statically, the instruction is executed once everything required is already available. 
Remember that VLIW architecture uses a singular instruction bundle called Very Long Instruction, which results in the requirement of only one **Program Counter**.

In VLIW, it is possible to parallelize WAR and WAW in the same clock if they are on different units. WAW can only occur in the specific case of two consecutive instructions that write to the same register, and offsetting by one cycle should not be a problem. By definition, there are no WARs.


### VLIW: Pros and Cons

Pros:

* Simple HW: simpler hardware compared with superscalar processors. The hardware does not need sophisticated scheduling logic since it simply executes each operation according its position within the word without checking for dependencies among them.
* Easy to extend the # FUs: tilize all FUs in each cycle (in VLIW) as much as possible to reach better ILP and therefore higher parallel speedups.
* Good compilers can effectively detect parallelism

Cons:

* Huge number of registers to keep active the FUs
* Large data transport capacity between FUs and register files and between register files and memory
* High bandwidth between i-cache and fetch unit: the bus bandwidth should be capable of transferring an instruction that contains n operations, requiring a bandwidth of `n*(operation size)`.
* large code size since each instruction word contains many operations. 
* binary compatibility


## Static Scheduling methods

There are many different Static Scheduling methods that can be applied:

* **Simple code motion**: just moving code instructions during scheduling.
* **Loop unrolling & loop peeling**: Unrolling exposes more computation that can be scheduled to minimize stalls. If there aren't loop-carried dependencies we can "unroll" the interations of any loop and perform them in a parallel way.
* **Software pipeline**
* **Trace scheduling**


### Loop unrolling 


![](images/74458441b28597113cb27d5c03c4b802.png)

Software pipelining pushes even further this idea, permetting the loop iterations that are executed parallelly to also be pipelined.

![](images/614a790f0c978689b66fe0c66a2d6073.png)


```assembly
loop:   ld f1, (r1)
        ld f2, 0(r2)
        fmul f1, f1, f1
        fadd f1, f1,f2
        st f1, 0(r3)
        addi r1, r1,4
        addi r2, r2,4
        addi r3, r3,4
        bne r3, r4, loop
```

Unroll the loop by one iteration (so two iterations 
        of the original loop are performed for every branch in 
        the new assembly code)
        ● You only need to worry about the steady-state code 
        in the core of the loop (no epilogue or prologue)
        ● schedule the assembly code for the 3-issue VLIW 
        machine in the following table by using the listbased scheduling with ASAP

```assembly
loop:   ld f1, (r1)
        ld f3, 4(r1)
        ld f2, 0(r2)
        ld f4, 4(r2)
        fmul f1, f1, f1
        fmul f3, f3, f3
        fadd f1, f1,f2
        fadd f3, f3, f4
        st f1, 0(r3)
        st f3, 4(r3)
        addi r1, r1,8
        addi r2, r2,8
        addi r3, r3,8
        bne r3, r4, loop
```



### Trace Scheduling

![](images/91c3f7c234a0cb20bd60a6afc805d6db.png)

A trace is a portion of the code/control flow ... it's basically a branch path.

Use profiling feedback or compiler heuristics to find common branch paths

Trace scheduling cannot proceed beyond a loop barrier

Traces scheduling schedules traces in order of decreasing probability of being executed

So, most frequently executed traces get better schedules
Traces are scheduled as if they were basic blocks.







![](images/c5ffc356e79e344664883601da7835bc.png)

It then **adds compensation code at the entry and exit of each trace to compensate for any effects that out-of-order execution may have had**.

| NOP | Integer ALU (1 cc) | Memory Unit (2 cc) | FPU (3 cc) |
| :---: | :---: | :---: | :---: |
| C1 | NOP | ld f1, 0(r1) | NOP |
| C2 | addi r1, r1,8 | ld f3, 4(r1) | NOP |
| C3 | NOP | ld f2, 0(r2) | fmul f1, f1, f1 |
| C4 | addi r2, r2,8 | ld f4, 4(r2) | fmul f3, f3, f3 |
| C5 | NOP | NOP | NOP |
| C6 | NOP | NOP | fadd f1, f1, f2 |
| C7 | NOP | NOP | fadd f3, f3, f4 |
| C8 | NOP | NOP | NOP |
| C9 | NOP | st f1,0(r3) | NOP |
| C10 | addi r3, r3,8 | st f3, 4(r3) | NOP |
| C11 | bne r3, r4, loop | NOP | NOP |





Loop unrolling and software pipelining are two static scheduling techniques. 

The main difference between them is that loop unrolling replicates the entire code of the loop, while software pipelining can be thought of as "symbolic" loop unrolling. This means that loop unrolling needs to add some startup code before the loop as well as finish-up code at the end, whereas software pipelining does not. 

The amount of registers used is not related to the type of scheduling, but only to the architecture used. 

Loop unrolling needs to repeat the computation at each iteration, which results in paying the cost of overhead and wind-down more times than software pipelining. 

Both techniques have lower power consumption compared to dynamic scheduling techniques. Thus, power consumption does not depend on the type of scheduling technique used.




Loop Unrolling performance can be enhanced by rescheduling. Anticipating the next loop iteration at a scheduling level helps in this. This is done by anticipating operations to free slots in the pipeline that can be used to start up the next iteration. Compensating instructions may need to be added.


  

Trace Schuedling use? 

A trace is a loop-free sequence of basic blocks, embedded in the control graph. Trace scheduling aims to execute and profile the application to find the most probable sequence to be executed. The sequence of probable blocks is then considered as a single one, allowing for heavy rescheduling of the entire operation set to improve performance.

  



Compensating code refers to the code that is put in place to address any potential issues that may arise in a computer program. An example of this is when a counter is decreased after it has been previously incremented. This type of compensating code is often used to mitigate the effects of errors, bugs or other issues that may impact the functionality of a program. By anticipating potential problems beforehand and providing compensating code, developers can help ensure that their code remains stable and reliable, even in the face of unexpected issues or errors.





