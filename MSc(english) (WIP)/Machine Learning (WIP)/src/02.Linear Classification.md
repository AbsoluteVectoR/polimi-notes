# Linear Classification


Linear Classification Models:

Direct Approaches:

-   Least Squares for Classification
-   The Perceptron Algorithm

Probabilistic Discriminative Approach:

-   Logistic Regression


To learn an approximation of function $f(x)$ that maps input $x$ to a discrete class $C_k$ (with $k = 1, ..., K$) from a dataset $D$, we need to use a classification algorithm. 

Parametric discriminant function:

-   Model a parametric function that maps input to classes
-   Learn parameters from data

Probabilistic discriminative approach:

-   Design a parametric model of $p(C_k|x)$
-   Learn model parameters from data

Probabilistic generative approach (more ambitious and difficult in general):

-   Model $p(x|C_k)$ and class priors $p(C_k)$
-   Fit models to data
-   Infer posterior with Bayesâ€™ rule: $p(C_k|x) = \frac{p(x|C_k) p(C_k)}{p(x)}$



In linear classification, we will use generalized linear models

$$
f(\mathbf{x}, \mathbf{w})=f\left(w_0+\sum_{j=1}^{D-1} w_j x_j\right)=f\left(\mathbf{x}^T \mathbf{w}+w_0\right)
$$

<- check the different of the notation

$f(\cdot)$ is not linear in $w$ due to the (non-linear) activation function $f$, because its output is either a discrete label or a probability value. $f(\cdot)$ partitions the input space into decision regions whose boundaries are called decision boundaries or decision surfaces. These decision surfaces are linear functions of $x$ and $w$, as they correspond to $x^T w + w_0 = \text{const}$.

Generalized linear models are more complex to use compared to linear models (both from a computational and analytical perspective).





A common encoding for two-class problems is a binary encoding: $t \in {0,1}$. $t=1$ encodes positive class and $t=0$ encodes negative one. With this encoding, $t$ and $f(x)$ represent the probability of positive class.

A possible alternative encoding for two-class problems is $t \in {-1,1}$. This encoding is convenient for some algorithms.

When the problem has $K$ classes a choice could be to use $0,1,2 .. k-1$ but that's not the optimal one and most used. A typical choice is a vector of length $K$, with a 1 in the position corresponding to the encoded class. With this encoding, $t$ and $f(x)$ represent the probability density over the classes. As an example, a data sample that belongs to class 4 of a problem with $K=5$, is encoded as $t=(0,0,0,1,0)^T$


Single class 


![](c1dd425f6c0177293c4650994562ed93.png)

In a multi-class problem we have K classes

![](51ab2eda2b9338cc3c7fda0d3b27bd71.png){width=50%}

One-versus-the-rest approach 

One-versus-one approach


A simple solution for multiple classes is to use K linear  
discriminant functions


studiare fino al Perceptron 
