# Linear Classification


The goal in classification is to take an input vector x and to assign it to one of K discrete classes 


Linear Classification Models:

Direct Approaches:

-   Least Squares for Classification
-   The Perceptron Algorithm

Probabilistic Discriminative Approach:

-   Logistic Regression


To learn an approximation of function $f(x)$ that maps input $x$ to a discrete class $C_k$ (with $k = 1, ..., K$) from a dataset $D$, we need to use a classification algorithm. 

Parametric discriminant function:

-   Model a parametric function that maps input to classes
-   Learn parameters from data

Probabilistic discriminative approach:

-   Design a parametric model of $p(C_k|x)$
-   Learn model parameters from data

Probabilistic generative approach (more ambitious and difficult in general):

-   Model $p(x|C_k)$ and class priors $p(C_k)$
-   Fit models to data
-   Infer posterior with Bayes' rule: $p(C_k|x) = \frac{p(x|C_k) p(C_k)}{p(x)}$



In linear classification, we will use generalized linear models

$$
f(\mathbf{x}, \mathbf{w})=f\left(w_0+\sum_{j=1}^{D-1} w_j x_j\right)=f\left(\mathbf{x}^T \mathbf{w}+w_0\right)
$$

<- check the different of the notation

$f(\cdot)$ is not linear in $w$ due to the (non-linear) activation function $f$, because its output is either a discrete label or a probability value. $f(\cdot)$ partitions the input space into decision regions whose boundaries are called decision boundaries or decision surfaces. These decision surfaces are linear functions of $x$ and $w$, as they correspond to $x^T w + w_0 = \text{const}$.

Generalized linear models are more complex to use compared to linear models (both from a computational and analytical perspective).


A common encoding for two-class problems is a binary encoding: $t \in {0,1}$. $t=1$ encodes positive class and $t=0$ encodes negative one. With this encoding, $t$ and $f(x)$ represent the probability of positive class.

A possible alternative encoding for two-class problems is $t \in {-1,1}$. This encoding is convenient for some algorithms.

When the problem has $K$ classes a choice could be to use $0,1,2 .. k-1$ but that's not the optimal one and most used. A typical choice is a vector of length $K$, with a 1 in the position corresponding to the encoded class. With this encoding, $t$ and $f(x)$ represent the probability density over the classes. As an example, a data sample that belongs to class 4 of a problem with $K=5$, is encoded as $t=(0,0,0,1,0)^T$


Single class 


![](c1dd425f6c0177293c4650994562ed93.png)

In a multi-class problem we have K classes

![](51ab2eda2b9338cc3c7fda0d3b27bd71.png){width=50%}

consider the extension of linear discriminants

One-versus-the-rest approach 

Consider the use of K-1 classifiers each of which solves a two-class problem of separating points in a particular class Ck from points not in that class. There are regions in input space that are ambiguously classified, as we can see in the left-hand diagram.


One-versus-one approach

An alternative is to introduce K(K - 1)/2 binary discriminant functions, one for every possible pair of classes. Each point is then classified according to a majority vote amongst the discriminant functions. However, this too runs into the problem of ambiguous regions, as illustrated in the right-hand diagram


A simple solution for multiple classes is to use K linear  
discriminant functions

# K linear discriminant functions

$$y_k(x)=x^T w_k + w_{k0}$$
where $k=1,\dots , K$ and then assign $x$ to $C_k$ iff $y_k(x)>y_j(x)$ with $\forall j \ne k$. This eliminates ambuiguity. 

Also we can use feature space again like in linear regression: 
we apply a non-linear transformation to map the input space into a feature space. 


Least squares could be an approach for classification but is problematic in some cases. 

Least square is very sensitive to outliers. Least square tries to find a line which is the most close to all points. It does so evaluating the square distance between the samples and the line. It means that an outlier will have a greater impact on the line position because it will be more distant with respect to the probable samples.

This because one of the assumptions of least squares is to have a linear noise which follows a Gaussian distribution: the model generating the data is a linear model plus a noise with gaussian distribution with some unknown standard deviation (hopefully not too big). If we have outliers, it's like the noise is not following any gaussian distribution. So this, in practice, makes least square not able to find good decision boundaries.


![](295cc25bc95a7d499afa3d27f7d6e3ea.png){width=50%}


For this reason we use Perceptron tecnique

The perceptron is an algorithm for online supervised learning of binary classifiers. The algorithm tries to find a threshold function: a function that maps its input x (a real-valued vector) to an output value 

Online means that it is an iterative approach which calculate the solutions with multiple steps

We use another loss function: 

$$
L_P(w)=-\sum_{n \in M} w^T \Phi\left(x_n\right) t_n
$$

It's designed to penalize the outliers:

- samples classified correctly do not contribute to $L$
- each misclassified sample  contributes as $w^T \Phi\left(x_i\right) t_i$

# Logicistic regression

While Least squares method is highly sensitive to outliers, logistic regression is not.

Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary variable. So it is capable of resolve two-class classification. Logistic regression is a discriminative model so we model directly the posterior probability $ P(C_k|\Phi)$. In detail we use a logistic sigmoid function. 


$$
P\left(C_1 \mid \Phi\right)=\sigma\left(w^T \Phi\right)=\frac{1}{1+e^{-w^T \Phi}}$$


$$ P\left(C_2 \mid \Phi\right)=1-P\left(C_1 \mid \Phi\right) = 1 - \frac{1}{1+e^{-w^T \Phi}}$$


![](29855cac56ce2cc20055c307d9dadb52.png)


Example: 


![](00798e9461c2060d12af7e8a5cd002b9.png)

$$
\begin{array}{ll}
P\left(t_1=1 \mid \phi_1, w\right) & P\left(t_3=1 \mid \phi_3, w\right) \\
P\left(t_2=0 \mid \phi_2, w\right) &
\end{array}
$$

So we can take as loss function the product of all the probabilities $P(t_n)$. 

$$
R_L=\prod_{i=1}^3 P_i
$$

Then we want to maximise this probability. 
As always we convert the maximization into a minimization of the negative log-likelihood: 

$$
L(\mathbf{w})=-\ln p(\mathbf{t} \mid \mathbf{X}, \mathbf{w})=-\sum_{n=1}^N\left(t_n \ln y_n+\left(1-t_n\right) \ln \left(1-y_n\right)\right)=\sum_{n=1}^N L_n
$$
computing the derivative (gradient): 

$$
\begin{gathered}
\frac{\partial L_n}{\partial y_n}=\frac{y_n-t_n}{y_n\left(1-y_n\right)}, \quad \frac{\partial y_n}{\partial \mathbf{w}}=y_n\left(1-y_n\right) \phi_n, \quad \frac{\partial L_n}{\partial \mathbf{w}}=\frac{\partial L_n}{\partial y_n} \frac{\partial y_n}{\partial \mathbf{w}}=\left(y_n-t_n\right) \phi_n \\
\nabla L(\mathbf{w})=\sum_{n=1}^N\left(y_n-t_n\right) \phi_n
\end{gathered}
$$
and also we can extend the logistic regression of a binary classification into a multiclass logistic regression:

$$
\nabla L_{\mathbf{w}_j}\left(\mathbf{w}_1, \ldots, \mathbf{w}_K\right)=\sum_{n=1}^N\left(y_{n j}-t_{n j}\right) \phi_n
$$

### Logistic regression as a generalization of perceptron

Perceptron can be seen as a special case of logistic regression, where the logistic function is replaced by a step function.


![](c34c7890610c73a24a85accd3a0bba13.png)


## Exe del 14/03 


In Euclidean geometry, **linear separability** is a property of two sets of points "Point (geometry)").


- Discriminant function approach: 
	- Least square approach
	- Perceptron 
	- 1-NN 
- Probabilistic discriminant approach: 
	- Logistic Regression 
- Probabilistic generative approach: it can generates new sample from joint probability  
	- Naive-Bayes 



The perceptron is an example of linear discriminant model. It is, unlike least squares that  
has a closed form solution, an online linear classification algorithm.

$$y(\mathbf{x}) = f\left(\mathbf{w}^T \phi(\mathbf{x})\right)$$

where $f$ is a step function:
$$
f(a)= \begin{cases}+1 & a \geq 0 \\
-1 & a<0\end{cases}$$


The algorithm finds the separating hyperplane by minimizing the distance of misclassified points to the decision boundary. Sum of the distances of misclassified points from the current separating hyperplane. 

$$L_P(\mathbf{x}) = -\sum_{n \in M} t_n (\mathbf{w}^T \phi(\mathbf{x}_n))$$

Batch Gradient Descent, Mini-Batch Gradient Descent and Stochastic Gradient Descent are optimization algorithms used in machine learning to calculate the parameters of a model. These algorithms are used to minimize the cost function of a model by adjusting its parameters.

- Batch Gradient Descent: This algorithm updates the parameters using the average of all training examples at each iteration. It calculates the gradient for all training examples and then takes an average before updating the weights. The advantage of this method is that it converges faster than other methods because it uses all data points at once, but it requires more memory as it needs to store all data points in memory.

- Mini-Batch Gradient Descent: This algorithm updates the parameters using a subset (or mini-batch) of training examples at each iteration. It divides the entire dataset into small batches and computes gradients on these batches instead of computing them on whole dataset like batch gradient descent. The advantage is that it reduces variance in parameter update which can lead to more stable convergence, while also being less computationally expensive than batch gradient descent.

- Stochastic Gradient Descent: This algorithm updates the parameters using only one training example at each iteration. It randomly selects one sample from dataset and computes its gradient with respect to loss function before updating weights.The advantage is that it requires less memory as only one sample needs to be stored in memory during computation, but may converge slowly due to high variance caused by random selection.

In summary:

- Batch GD uses all samples per epoch
- Mini-batch GD uses subsets or mini-batches per epoch
- Stochastic GD uses single samples per epoch

Perceptron can only use Stochastic Gradient Descent. 

If the dataset fed to a perceptron is not linearly separable, the Perceptron algorithm will not terminate. Otherwise, if the dataset is linearly separable, Perceptron will converges in a finite number of steps. 

Many hyperplanes exist and Perceptron converges on one, and it converges depending on: 
1) initialization 
2) order of how the data is processed by the algorithm 

How can I find the hyperplane? 

$$
\begin{gathered}
\operatorname{sign}\left(w_0+x_1 w_1+x_2 w_2\right)=0 \\
\left(x_2=\frac{-w_1 x_1-w_0}{w_2}\right)
\end{gathered}
$$

CONFUSION MATRIX COPY HERE!! <- remembers me something from [Probabilitá e Statistica per l'Informatica](Probabilitá%20e%20Statistica%20per%20l'Informatica.md) . What? 

# Logistic regression uses Sigmoid function. 

How is logistic regression related to Bernoulli Distribution?

In case of Logistic regression to find the separating hyperplane (decision boundary) you can plot $\sigma (w^T x)=\frac{1}{2}$ .  


# Naive Bayes 

I can factor the Bayes formula and obtain the individual probabilities ... 
It's simple to train than "the full model" where many features are inter-related each other by some unknown model.  Obviously it's a semplification ... and this can lead to bad predictions ... 

# K-Nearest Neighbor 

![](009511f890486d7db7fb45501c958e87.png){width=50%}


Non-parametric models do not involve any weights and require no training. They simply query over the dataset to make predictions for both regression and classification problems.

### Design Choices for Matrix

When designing a non-parametric model, there are several choices to consider regarding the matrix:

- Euclidean distance
- K-nearest neighbors (where k > 1)

In a classification problem using KNN, if k is greater than 1, we can use mode class. This means selecting the class that appears most frequently among the neighbors.
For linear regression using KNN, if k is greater than 1, we can take an average of the targets of all neighboring data points.





KNN could be computationally heavy to make inference (find the neighbours) on a large dataset. A parametric method can be train on a small dataset and then make inference on the large one with the already developed weigths.

Remember that non parametric method has not training: every time you have constraints on memory or computation you prefer not parametric method. 



<hr class="__chatgpt_plugin">

role::assistant

# Linear Classification


The goal in classification is to take an input vector x and to assign it to one of K discrete classes 


Linear Classification Models:

Direct Approaches:

-   Least Squares for Classification
-   The Perceptron Algorithm

Probabilistic Discriminative Approach:

-   Logistic Regression


To learn an approximation of function $f(x)$ that maps input $x$ to a discrete class $C_k$ (with $k = 1, ..., K$) from a dataset $D$, we need to use a classification algorithm. 

Parametric discriminant function:

-   Model a parametric function that maps input to classes
-   Learn parameters from data

Probabilistic discriminative approach:

-   Design a parametric model of $p(C_k|x)$
-   Learn model parameters from data

Probabilistic generative approach (more ambitious and difficult in general):

-   Model $p(x|C_k)$ and class priors $p(C_k)$
-   Fit models to data
-   Infer posterior with Bayes' rule: $p(C_k|x) = \frac{p(x|C_k) p(C_k)}{p(x)}$



In linear classification, we will use generalized linear models

$$
f(\mathbf{x}, \mathbf{w})=f\left(w_0+\sum_{j=1}^{D-1} w_j x_j\right)=f\left(\mathbf{x}^T \mathbf{w}+w_0\right)
$$

<- check the different of the notation

$f(\cdot)$ is not linear in $w$ due to the (non-linear) activation function $f$, because its output is either a discrete label or a probability value. $f(\cdot)$ partitions the input space into decision regions whose boundaries are called decision boundaries or decision surfaces. These decision surfaces are linear functions of $x$ and $w$, as they correspond to $x^T w + w_0 = \text{const}$.

Generalized linear models are more complex to use compared to linear models (both from a computational and analytical perspective).


A common encoding for two-class problems is a binary encoding: $t \in {0,1}$. $t=1$ encodes positive class and $t=0$ encodes negative one. With this encoding, $t$ and $f(x)$ represent the probability of positive class.

A possible alternative encoding for two

<hr class="__chatgpt_plugin">

role::user

| Scenario                                                              | I prefer parametric or not parametric approach? |
|:--------------------------------------------------------------------- |:----------------------------------------------- |
| Large dataset (big data scenario);                                    | Parametric                                      |
| Embedded system or component with reduced computational capabilities. | Non parametric                                  |
| Prior information on data distribution                                | Parametric                  ?                   |
| Learning in a Real-time scenario                                      | ??                                              |



<hr class="__chatgpt_plugin">

role::assistant

# Linear Classification

Linear classification is a type of supervised learning where the goal is to assign an input vector x to one of K discrete classes. There are different approaches to linear classification, including direct methods like least squares and the perceptron algorithm, as well as probabilistic discriminative approaches like logistic regression.

In linear classification, we use generalized linear models that map input x to classes using a parametric function. The function partitions the input space into decision regions whose boundaries are called decision boundaries or decision surfaces. These decision surfaces are linear functions of x and w, where w are the learned parameters.

There are different ways to encode the classes in linear classification, including binary encoding for two-class problems and a vector of length K for problems with K classes. One approach for multiple classes is to use K linear discriminant functions, where each point is classified according to a majority vote among the discriminant functions.

Least squares can be used for classification, but it is sensitive to outliers. The perceptron algorithm is an online supervised learning algorithm that is less sensitive to outliers. Logistic regression is a probabilistic discriminative approach that uses a logistic sigmoid function to model the posterior probability of the classes.

Naive Bayes is a probabilistic generative approach that models the joint probability of the features and the classes. K-nearest neighbor is a non-parametric approach that makes predictions based on the k-nearest neighbors in the training set. The choice between parametric and non-parametric approaches depends on factors like the size of the dataset, computational capabilities, and prior knowledge of the data distribution.

<hr class="__chatgpt_plugin">

role::user

