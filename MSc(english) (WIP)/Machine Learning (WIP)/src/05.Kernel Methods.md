# Kernel Methods

Kernel methods are a family of **non-parametric** techniques.

With parametric method a certain hypothesis in the hypothesis space is defined by the combination of values of the learnable parameters. For example, linear regression is a parametric method, where each hypothesis is defined by the value of the parameters associated with each feature plus the constant term.



With non-parametric methods we have no explicit parameters. In parametric methods the training set is used in the training phase to learn the parameters. Then for the prediction phase the training set is not used because all the relevant information are encoded in the learned model.


Sometimes linear separators cannot be found in the input space in machine learning.
A solution is to map the input space to a feature space that allows a linear separator to be found. This can be done by adding features. 
![](images/Pasted%20image%2020230331175702.png)


But computing the feature mapping for a large number of input variables is computationally infeasible. This is where kernel methods come in, as they don't require explicit computation of the feature mapping. Although they are expensive, they are still computationally feasible.

The kernel function is defined as the scalar product between the feature vectors. 
This is useful since often it's possible to compute the Kernel functions without actually computing the feature vectors. 

$$k(x,x')=\phi (x)^T \phi (x')$$

Math trick 

It is possible to rework the representation of linear models to replace all the terms that involve 𝜙𝜙 x with other terms that involve only 𝑘𝑘 x,⋅

In other words, the output of linear model can be computed only on the basis of the similarities between data samples (computed with the kernel function)

Many linear models for regression and classification can be reformulated in terms of dual representation, in which the kernel function arises naturally. We want this in order to be able to apply the kernel trick. In practice we want to describe our model not using feature but with a kernel. For every linear model exist a dual representation involving kernels. Let’s take as an example ridge regression.

f the data is not linearly separable in the original, or input, space then we apply transformations to the data, which map the data from the original space into a higher dimensional feature space. The goal is that after the transformation to the higher dimensional space, the classes are now linearly separable _in this higher dimensional feature space_.


Kernel trick 

In real applications, there might be many features in the data and applying transformations that involve many polynomial combinations of these features will lead to extremely high and impractical computational costs.

The “trick” is that kernel methods represent the data only through a set of pairwise similarity comparisons between the original data observations **x** (with the original coordinates in the lower dimensional space), instead of explicitly applying the transformations _ϕ_(**x**) and representing the data by these transformed coordinates in the higher dimensional feature space.

Remember, our data is only linearly separable as the vectors _ϕ_(**x**) in the higher dimensional space, and we are finding the optimal separating hyperplane in this higher dimensional space _without having to calculate or in reality even know anything about ϕ(x)._


Dual representation Is computationally convenient when  is very large or even infinite

We can represent features expansion that include billions of elements with very simple kernel which need few operation to be computed. In this way we have constructed a memory based method which doesn’t use both features and weights, but it exploit the training data only to predict new samples.

Computationally speaking is very very conveniant. 


**Gaussian Kernel**

