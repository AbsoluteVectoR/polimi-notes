# Bias-Variance

Is the loss function a good way to evaluate a model?
No, the loss function is only telling us how good the model is on the data used to train the model. But it's telling nothing about unseen data. 
Supervised learning is not used to explain seen data! 

Bias-Variance Decomposition

The Bias-Variance is a framework to analyze the performance of models.

We can decompose the expected square error (which indicates the performance of a model) as:

$$
\begin{aligned}
\mathbb{E}\left[(t-y(\mathbf{x}))^2\right] & =\mathbb{E}\left[t^2+y(\mathbf{x})^2-2 t y(\mathbf{x})\right] \\
& =\mathbb{E}\left[t^2\right]+\mathbb{E}\left[y(\mathbf{x})^2\right]-\mathbb{E}[2 \operatorname{tr}(\mathbf{x})] \\
& =\mathbb{E}\left[t^2\right] \pm \mathbb{E}[t]^2+\mathbb{E}\left[y(\mathbf{x})^2\right] \pm \mathbb{E}[y(\mathbf{x})]^2-2 f(\mathbf{x}) \mathbb{E}[y(\mathbf{x})] \\
& =\operatorname{Var}[t]+\mathbb{E}[t]^2+\operatorname{Var}[y(\mathbf{x})]+\mathbb{E}[y(\mathbf{x})]^2-2 f(\mathbf{x}) \mathbb{E}[y(\mathbf{x})] \\
& =\operatorname{Var}[t]+\operatorname{Var}[y(\mathbf{x})]+(f(\mathbf{x})-\mathbb{E}[y(\mathbf{x})])^2 \\
& =\underbrace{\operatorname{Var}[t]}_{\sigma^2}+\underbrace{\operatorname{Var}[y(\mathbf{x})]}_{\text {Variance }}+\underbrace{\mathbb{E}[f(\mathbf{x})-y(\mathbf{x})]^2}_{\text {Bias}^2}
\end{aligned}
$$

Visualization: 

![](c335c1050b5f93c3cd36bf594f9cf3c7.png){width=50%}


Typically there is a trade-off between bias-variance:

- bias measures the difference between truth and what we expect to learn
- variance measures the difference between each model learned from a particular dataset and what we expect to learn


The more complex is the model and the higher will be the variance: 

![](dcd091ba409e0c0ac14cf1dd694cfb16.png){width=50%}


