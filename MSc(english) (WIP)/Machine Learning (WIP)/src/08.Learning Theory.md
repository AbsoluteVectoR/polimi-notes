
# Computational Learning Theory


> "How many samples needed to converge to successful hypothesis?"
> "How many misclassified samples until successful convergence?"

Training error and true error are two different types of errors:

- **Training error**: error rate which measures how well the model **fits the training data**
- **True error**: true error, on the other hand, refers to the expected or average prediction error over all possible input values in an unseen dataset.

We cannot know the true error because we don't know the true distribution of data that the algorithm will work on. Instead, we can measure the model performance on a known set of training data (the "empirical" risk) and try to make an estimation of the true error.


## No free lunch theorem 

No free launch theorem 

"you can't learn from nothing"

There is no such thing as a winner-takes-all in ML! In ML we always operate under some assumptions!



## PAC-Learning

**Probably Approximately Correct** learning is a framework which gives us a mathematical pov of Machine Learning. 

(a concept aka the function which maps input with outputs)

A **concept class** is a set of possible concepts that our model can choose from when making predictions. It represents all possible functions or mappings that could explain the observed data.

A class $C$ of possible target concepts $c$ is **PAC-learnable** by a learner $L$ using $H$ (the hypotesis space) if for all $c \in C$, for any distribution $P(X), \varepsilon$ (such that $0<\varepsilon<1 / 2$ ), and $\delta$ (such that $0<\delta<1 / 2$ ), $L$ will with a probability at least $(1-\delta)$ output a hypothesis $h \in H$ such that error $_{\text {true }}(h) \leq \varepsilon$, in time that is polynomial in $1 / \varepsilon, 1 / \delta, M$, and size(c).

> "A sufficient condition to prove PAC-learnability is proving that a learner $L$ requires only a polynomial number of training examples and processing time for each example is polynomial."

If we have a concept class that is PAC-learnable it means that is not hard to learn. 



There is relationship between train error and prediction error (true error)

- Close relationship between the two when under-fitting
- Relationship lost when over-fitting

**Our goal is to build confidence interval for prediction error given the knowledge of train error. In particular, we build an upper bound (function of the training loss) of the true loss.**
One thing to keep in mind: confidence interval grows when hypothesis space grows.

### Version space 

Subset of the hypotesis space `H` that contains all hypotheses with 0 training error.
In practice usually we need to work outside of the version space.


In a finite hypotesis space $\mathrm{H}$ using the Hoeffding bound.  if we bound this probability to be $<=\delta$ we can get the value of $\varepsilon$ and decompose the true error in the two components given by the bias and the variance. e can now bound the true error
$$
L_{\text {true }}(h) \leq \underbrace{L_{\text {train }}(h)}_{\text {Bias }}+\underbrace{\sqrt{\frac{\ln |H|+\ln \frac{1}{\delta}}{2 N}}}_{\text {Variance }}
$$
Interpretation:

-   for high `|H|` --> low bias, high variance
-   for low `|H|` --> high bias, low variance

We have now a theoretical formula that confirms what so far we only said empirically by looking at the different techniques.


### Hoeffding Bound


Hoeffding Bound is a tool to build confidence interval, in particular it permits us to build an upper bound with at least $1-\delta$ confidence. Note that we found the bias and variance decomposition we previously saw in the course:

$$\operatorname{error}_{\text {true }}(h) \leq \operatorname{error}_{\mathcal{D}}(h)+\sqrt{\frac{\ln |H|+\ln \frac{1}{\delta}}{2 N}}$$


or in other terms: 
$$
\mathcal{L}(\hat{h}) \leq \tilde{\mathcal{L}}(\hat{h})+ L\sqrt{\frac{\log|H| + \log(\frac{1}{\delta})}{2N}}
$$

where:

- $\mathcal{L}(\hat{h})$ is the true error rate (risk) of our model, which measures how well it performs on new, unseen data points.
- $\tilde{\mathcal{L}}(\hat{h})$ is the empirical error rate (training loss) of our model, which measures how well it fits to a given set of training data.
- $L$ is a constant that depends on the range or magnitude of our output values. It can be thought of as an upper bound on how much our predictions can deviate from their true values.
- $J$ represents the size of our test set, i.e., how many samples we have available to evaluate our model's performance on new data points.
- $\delta$ represents confidence level or probability threshold we want to achieve with respect to this inequality.

- $\mathcal{L}(\hat{h})$ tells us how many mistakes our robot might make when looking at new pictures.
- $\tilde{\mathcal{L}}(\hat{h})$ tells us how many mistakes our robot made when we showed it some example pictures during training.
- $L$ is like an upper limit for how much our robot can get things wrong.
- $J$ tells us how many new pictures we have available to test our robot with.
- $\delta$ represents confidence level or probability threshold we want to achieve with respect to this inequality.

So basically, this formula says in a math way the things that we already know: 

- Larger hypotheses space $\implies$ larger bound (variance)
- Increasing $N$ $\implies$ reduced bound (variance)
- Large $|H|$: low bias, high variance
- Small $|H|$: high bias, low variance


#### Consistent Learning

Consistent learning with $\hat{L}(\hat{h})=0$ : 

$$\mathcal{L}(\hat{h}) \leq \frac{\log |\mathcal{H}|+\log \left(\frac{1}{\delta}\right)}{N} \quad \text { w.p. } \quad 1-\delta$$



#### Agnostic Learning

So far, we assumed that $c \in H$ and that the learner $L$ will always output a hypothesis $h$ such that $\operatorname{error}_{\mathcal{D}}(h)=0$ , actually $L$ will output a hypothesis $h$ such that $\operatorname{error}_{\mathcal{D}}(h)>0$.
This situation is called "agnostic" learning


## VC Dimension

The Vapnik-Chervonenkis dimension, VC(H)

A linear classifier is a model that tries to draw a straight line (or plane, or hyperplane) to separate two groups of data points. In a 2D input space, this means we're trying to draw a line that separates one group of points from another.

The VC dimension for this linear classifier in 2D is 3. This means that if we have up to three points in our dataset, we can always find some way to draw a line between them so that all the points on one side are in one group and all the points on the other side are in another group.

![](images/f7dbde16427ae1ca20d2e20c3ada1913.png)

Now let's move on to M-D input space. The "M" here stands for any number of dimensions - it could be 3D, 4D, or even higher!

![](images/Pasted%20image%2020230424181820.png)

In general, as we increase the number of dimensions in our input space, it becomes harder and harder to visualize how our data is separated by a linear boundary like a plane or hyperplane. But mathematically speaking, it turns out that the VC dimension for a linear classifier in M-D input space is simply M+1.

So if we have an M-dimensional dataset and use a linear classifier model with this formula: y = w0 + w1x1 + w2x2 + ... + wm-1xm

Then according to Vapnik-Chervonenkis theory (VC), there will always exist some combination of weights (w0 through wm-1) such that any set of up to M+1 data points can be perfectly classified into two groups using this model.



What is important in determining the size of `H`? Before that, some concepts:

-   dichotomy: given a set `S` of points a dichotomy is a partition of that set `S` in 2 disjunct subsets
-   shattering: we say that a set `S` is shattered by an hypothesis space `H` if and only if for every dichotomy in `S` there exist some hypotheses in `H` consistent with this dichotomy. Basically `H` is shattering `S` if it is able to perfectly classify all the examples in `S` independently of how they are labelled.

