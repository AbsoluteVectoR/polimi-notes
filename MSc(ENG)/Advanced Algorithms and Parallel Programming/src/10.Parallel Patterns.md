# Parallel Patterns

A Parallel Pattern is a recurring combination of task distribution and data access that solves a specific problem in parallel algorithm design. Patterns are universal, they can be used in any parallel programming system.
Parallel patterns will be classified under these macro-classes: 

- nesting pattern
- parallel control patterns 
- parallel data management patterns
- other patterns

## Nesting Pattern

![](images/6e85937742c3f7a7f16b97218d26de4e.png){width=50%}


## Parallel control patterns 

Serial Control Patterns are the classical ones: 
- sequence pattern
- iteration pattern 
- selection pattern 
- recursion pattern 

### Fork-Join pattern 

![](images/d108328a4f478877087adea69e52904e.png){width=50%}

The fork-join pattern is a common parallelization technique used to decompose a sequence pattern into smaller subproblems that can be solved in parallel. The basic idea is to split the original problem into multiple tasks and then combine the results of these tasks to obtain the final solution. 

### Map 

Generally it's difficult to extract parallelism from loops, but we can always do some analysis and find out something. We can parallelize this serial pattern using a map. Obviously only when each iteration is independent of others. 

![](images/4b36e51f055c1455c4b15420c6dc95c5.png){width=50%} 

### Stencil

An generalization of the map: a stencil is a function which accesses a set of "neighbors". These inputs are a set of fixed offsets relative to the output position.
Stencils can operate on one dimensional and multidimensional data so the neighborhoods can range from compact to sparse, square to cube, and anything else.

![](images/8d640d2a1c2719331b535d3578774418.png){width=50%} 

![](images/55dc48f4ee697522802e6e49e37721a9.png){width=50%}

Stencil is often used with iterative solvers or to evolve a system through time. 

#### Stencil and cache optimization 

Assigning rows to cores: 

- Maximizes horizontal data locality - Assuming vertical offsets in stencil, this will create redundant reads of adjacent rows from each core 

Assigning columns to cores: 

- Redundantly read data from same cache line - Create false sharing as cores write to same cache line

Assigning "strips" to each core can be a better solution in width: an optimization in a stencil computation that groups elements in a way that avoids redundant memory accesses and aligns memory accesses with cache lines. 

this to avoid problem cache sharing between multiple processors. 

Stencil and communication optimizations (ghost cells)

![](Pasted%20image%2020221130172717.png) 

green cells are called ghost cells for thread that computes the considered cell. 

Things to consider... What might happen to our ghost cells as we increase the number of threads? • the ghost cells to total cells ratio will rapidly increase causing a greater demand on memory What would be the benefits of using a larger number of ghost cells per thread? Negatives? • in the Game of Life example, we could double or triple our ghost cell boundary, allowing us to perform several iterations without stopping for a ghost cell update

Halo: set of all ghost cells Halo must contain all neighbors needed for one iteration Larger halo (deep halo) Trade off ons and more independence, but... • more redundant computation and more memory used Latency Hiding: Compute interior of stencil while waiting for ghost cell updates


To compute the value of a point, the stencil operation typically requires access to the values of its neighboring points. However, the points at the edges of the computational domain do not have all of their neighbors within the domain, which can cause problems when applying the stencil operation.

To address this issue, ghost cells are added to the edges of the computational domain to provide additional points that can be used by the stencil operation. These ghost cells are typically assigned the same value as the points on the edge of the domain, or are computed based on some other boundary condition. This allows the stencil operation to be applied consistently to all points in the domain, including the points at the edges. Ghost cells can improve the accuracy and efficiency of stencil parallel applications, and are commonly used in a variety of scientific and engineering computations.

Recurrence paralization 

![](Pasted%20image%2020221130172950.png)

This can still be parallelized! Trick: find a plane that cuts through grid of intermediate results Previously computed values on one side of plane Values to still be computed on other side of plane Computation proceeds perpendicular to plane through time (this is known as a sweep) This plane is called a separating hyperplane

Only if the recurrencies depends on constant offset of cells. 

Conclusion 92 Examined the stencil and recurrence pattern Both have a regular pattern of com unication and data access In both patterns we can convert a set of offset memory accesses to shifts Stencils can use strip-mining to optimize cache use C) Ghost cells should be considered when stencil data is distributed across different memory spaces 

### Reduction 

Reduction combines every element in a collection using an associative function. Why associative? The associative property allows us to ''split'' and change the order of operations of the reduction. addition, multiplication, maximum, minimum and boolean AND, OR, XOR are associative. 

![](images/ccfdb39d17c5182fcc7a7e04a83d2ce9.png){width=20%} 

Even a single processor can perform ''vectorization". For example without doing any parallelization we can still have a speed up because we can make an operation with 2 elements in a cycle (so we have speedup of 2): 

![](images/1192f17c158d56e0e2589896b86f8cee.png){width=20%}

In case of multiple 

![](images/66cd13f0fa0b23178d0e9e4e42e676a2.png)

Reduce example is the dot product is an essential operation in physics, graphics and videogames. 

### Scan

computes all partial reduction of a collection For every output in a collection, a reduction of the input up to that point is computed If the function being used is associative, the scan can be parallelized Parallelizing a scan is not obvious at first, because of dependencies to previous iterations in the serial loop A parallel scan will require more operations than a serial version

![](images/7d7c8da95f4e377690c674d847dd09cd.png){width=30%} 

Inclusive scan: includes current element in partial reduction - Exclusive scan: excludes current element in partial reduction, partial reduction is of all prior elements prior to current element

### Recurrence 

The recurrence parallel pattern is a more complex version of the map parallel pattern, in which the loop iterations can depend on one another. This means that, unlike in the map pattern, the elements of the input data are not necessarily independent of each other, and the outputs of some elements may be used as inputs to other elements. As a result, the order in which the elements are computed may be important, and the pattern typically involves the use of a serial ordering to ensure that the elements can be computed correctly.
This pattern is used to structure the parallel execution of code that involves recursion. 
Overall, the recurrence parallel pattern is a useful tool for improving the performance of recursive algorithms in parallel environments. It allows the parallel execution of recursive computations to be easily structured and managed, which can lead to significant performance improvements in many cases.

## Parallel Data Management Patterns

Serial Data Management Patterns are the classical ones: 

- stack allocation
- heap allocation
- objects 
- random read and write

While Parallel data management patterns are mainly: 


### Pack 

Pack is used to eliminate unused space in a collection Elements marked false are discarded, the remaining elements are placed in a contiguous sequence in the same order Useful when used with $\text{map}$ . 

![](images/526a61ae1f5b97aa9230000bfe65cb76.png)

Unpack is the inverse and is used to place elements back in their original locations. 

### Pipeline

Pipeline connects tasks in a producer-consumer manner, which is very common. A linear pipeline is the basic pattern idea, but a pipeline in a DAG is also possible. Pipelines are most useful when used with other patterns as they can multiply available parallelism.

### Geometric decomposition

Geometric Decomposition - arranges data into subcollections Overlapping and non-overlapping decompositions are possible This pattern doesn't necessarily move data, it just gives us another view of it

![](images/d69a30a8ba162a1cd55dde15e7ee104d.png) 

### Gather

We would have to keep data ''local'' and closer to the CPU since performance is often more limited by data movement than by computation. Transferring data across memory layers is costly: can take many cycles. 

Gather reads a collection of data given a collection of indices. Think of a combination of map and random serial reads. The output collection shares the same type as the input collection, but it share the same shape as the indices collection. 

![](images/a210fbc50ac0d58e357f6bef352e234c.png)

"combination of map with random reads" . "read locations provided as input"

### Scatter

"combination of map with random writes" . "write locations provided as input"

Scatter is the inverse of gather A set of input and indices is required, but each element of the input is written to the output at the given index instead of read from the input at the given index

This is different from Gather! Race conditions because write of same location are possible. Race conditions can occur when we have two writes to the same location!

![](images/19c330fc349f96fb923c7bf7a2cff7cf.png)

In case of collision we can have some rules like: 

- in case of associative and commutative operators can merge colliders. - we could associate to each value a priority. Example of this case in 3D graphics rendering. 

In case there aren't collisions the output is just a permutation, so no problem. 

### Split

![](527accb7d3c8a960b6c4133bee17b05c.png) 

Generalization of pack pattern, where the isn't information lose (like pack). 

There is also the ''inverse operation'' the unsplit. 

![](26d431ec07632d0e83ef29def142c91f.png)

generalization of split with "many classes''. Examples of this is Radix Sort or pattern classification (to classify values).

## Other Patterns 

- Superscalar Sequences: write a sequence of tasks, ordered only by dependencies 
- Futures: similar to fork-join, but tasks do not need to be nested hierarchically 
- Speculative Selection: general version of serial selection where the condition and both outcomes can all run in parallel 
- Workpile: general map pattern where each instance of elemental function can generate more instances, adding to the "pile" of work
- Search: finds some data in a collection that meets some criteria
- Segmentation: operations on subdivided, nonoverlapping, non-uniformly sized partitions of 1D collections
- Expand: a combination of pack and map 
- Category Reduction: Given a collection of elements each with a label, find all elements with same label and reduce them

## Part to revise 


The key to parallelism is independence. Map function should be ''pure'' and should not modify shared states, this means perfect independence and determinism, no data-races and no segfaults. 

Maps are very useful. Can sometimes "fuse" together the operations to perform them at once Adds arithmetic intensity, reduces memory/cache usage Ideally, operations can be performed using registers alone

![](images/4e9bd736a96f70997575efc15dc236f0.png)

Common strategy: 

1) Divide up the computational domain into sections 2) Work on the sections individually 3) Combine the results.

Possible methods: 

- Divide-and-conquer - Fork-join - Geometric decomposition - Partitions: data divided into non-overlapping equal-size regions - Segmentations: data divided into non-overlapping **not-uniform** regions

# Different way to store things in a parallizable way 

#### Array of structures AoS

an array containg the different instances of a data structure Most logical data organization layout. Extremely difficult to access memory for reads (gathers) and writes (scatters). 

#### Structure of arrays SoA

A single data structure that collects all the instances using many arrays: a single array for each property/attribute of a data structure. In each property/attribute array is stored all the values of all the different instances. tipically better for vectorization and avoidance of false sharing. Separate arrays for each structure-field, keeps memory access contiguos when vectorization is performed over structure instances. 

![](5f2d709303c357aa7e5a57784c37a744.png) 

The padding at the end indicates which is the size of a data structure. 

