<p>Why parallel programming?</p>
<ul>
<li>time saving</li>
<li>money saving</li>
<li>basically the ‘’big problems’’ can only solved by parallel
algorithms</li>
</ul>
<p>There is also automatic parallelization: where sequential algorithms
(at high level code) are automatically parallelized in high efficiency
assembly instructions.</p>
<p>The compiler is not able to know if for example 2 pointers of 2
arrays are pointing different region of RAM and are not overlapping.
Complete automatic parallelization is (at the moment?) not feasible</p>
<p>Tools are not able to extract all the available parallelism from a
specification designed to be executed in sequential way</p>
<p>at the moment Parallelization by hand. The programmer needs to give
hints to the tools… There are three critical aspects: Which type of
parallelism has to be considered How to design the parallel algorithm •
Trying to parallelize existing sequential algorithms • From scratch How
to provide information about the parallelism to the tools</p>
<p>There is not a single kind of parallelism: Instruction Parallelism
Data Parallelism - combination of them</p>
<p>single instruction, single data (all the single core architectures)
single instruction, multiple data (most of the modern GPUs) multiple
instruction, single data (experimental) multiple instruction, multiple
data (pthread or running in parallel on different data)</p>
<p>We can classify parallelism over different levels:</p>
<ul>
<li>bits level: It is very relevant in Hardware Implementation of
algorithm.</li>
<li>instructions level: Different instructions executed at the same time
on the same core. Supported by multiple execution units, pipeline,
vector, SIMD units etc. This type of parallelism can be easily extracted
by compilers</li>
<li>tasks level: Task: a logically discrete section of computational
work. Typically a program or program like set of instructions that is
executed by a processor supported by shared memory and cache mechanisms.
Usually difficult to be automatically extracted.</li>
</ul>
<p>Design a «good» parallel algorithm by extracting all the available
parallelism is not enough not all the extracted parallelism is
exploitable on a real architecture</p>
<p>We need to consider which parallelism is available on the considered
architecture Non suitable parallelism can introduce overhead</p>
<p>Real architectures differ so much that now compilers are not able to
fill the gap between an abstract model and real implementation Optimized
applications can not be generated starting from generic parallel code
Code extensions have been specialized for particular types of
applications/architectures</p>
<p><span class="math display">$$\begin{array}{|l|l|l|}
\hline \text { TECHNOLOGY } &amp; \text { TYPE } &amp; \text { YEAR }
&amp; \text { AUTHORS } \\
\hline \text { Verilog/VHDL } &amp; \text { Languages } &amp; 1984 /
1987 &amp; \text {US Government }\\
\hline \text { MPI } &amp; \text { Library } &amp; 1994 &amp; \text {MPI
Forum}\\
\hline \text { PThread } &amp; \text { Library } &amp; 1995 &amp; \text
{IEEE}\\
\hline \text { OpenMP } &amp; \text { C/Fortran Extensions } &amp;
1997  &amp; \text {OpenMP}\\
\hline \text { CUDA } &amp; \text { C Extensions } &amp; 2007 &amp;
\text {NVIDIA}\\
\hline \text { OpenCL } &amp; \text { C/C++ Extensions + API } &amp;
2008 &amp; \text {Apple}\\
\hline \text { Apache Spark } &amp; \text { API } &amp; 2014 &amp; \text
{Berkeley}\\
\hline
\end{array}$$</span></p>
<p><span class="math display">$$\begin{array}{l|l|l|l}
\hline \text { TECHNOLOGY } &amp; \text { Bit } &amp; \text {
Instruction } &amp; \text { Task } \\
\hline \text { Verilog/VHDL } &amp; \text { Yes } &amp; \text { Yes }
&amp; \text { No } \\
\hline \text { MPI } &amp; \text { (Yes) } &amp; \text { (Yes) } &amp;
\text { Yes } \\
\hline \text { PThread } &amp; \text { (Yes) } &amp; \text { (Yes) }
&amp; \text { Yes } \\
\hline \text { OpenMP } &amp; \text { (Yes) } &amp; \text { (Yes) }
&amp; \text { Yes } \\
\hline \text { CUDA } &amp; \text { (Yes) } &amp; \text { No } &amp;
\text { (Yes) } \\
\hline \text { OpenCL } &amp; \text { (Yes) } &amp; \text { No } &amp;
\text { Yes } \\
\hline \text { Apache Spark } &amp; \text { (Yes) } &amp; \text { No }
&amp; \text { (Yes)}
\end{array}$$</span></p>
<p><span class="math display">$$\begin{array}{|l|l|l|l|}
\hline \text { TECHNOLOGY } &amp; \text { SIMD } &amp; \text { MISD }
&amp; \text { MIMD } \\
\hline \text { Verilog/VHDL } &amp; \text { Yes } &amp; \text { Yes }
&amp; \text { Yes } \\
\hline \text { MPI } &amp; \text { Yes } &amp; \text { Yes } &amp; \text
{ Yes } \\
\hline \text { PThread } &amp; \text { Yes } &amp; \text { (Yes) } &amp;
\text { Yes } \\
\hline \text { OpenMP } &amp; \text { Yes } &amp; \text { Yes } &amp;
\text { Yels } \\
\hline \text { CUDA } &amp; \text { Yes } &amp; \text { No } &amp; \text
{ Yes) } \\
\hline \text { OpenCL } &amp; \text { Yes } &amp; \text { (Yes) } &amp;
\text { Yes } \\
\hline \text { Apache Spark } &amp; \text { Yes } &amp; \text { No }
&amp; \text { No } \\
\hline
\end{array}$$</span></p>
<p><img src="Pasted%20image%2020221028164604.png" /></p>
<h1 id="parallel-programming-pt-2">Parallel programming pt 2</h1>
<p>PThread is working very low level: threads management very explicit!
CUDA is very similar to OPENCL, since the last one is a try to extend
CUDA to all types of architectures and not only Nvidia.</p>
<p>We can classify technologies and libraries looking if the management
of memory/communications are implicitly/explicitly .</p>
<h1 id="parallel-programmings-patterns">Parallel programmings
patterns</h1>
